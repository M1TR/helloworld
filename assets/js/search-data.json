{
  
    
        "post0": {
            "title": "My Fast.ai Journey - Segmentation",
            "content": "from fastai.vision.all import * . Segmentation . Segmentation is similar to classification, except that we classify each individual pixel in the image. We&#39;ll use the CAMVID dataset which contains images from a car dashcam. Let&#39;s extract the dataset and take a look. . path = untar_data(URLs.CAMVID_TINY) path.ls() . (#3) [Path(&#39;/root/.fastai/data/camvid_tiny/labels&#39;),Path(&#39;/root/.fastai/data/camvid_tiny/images&#39;),Path(&#39;/root/.fastai/data/camvid_tiny/codes.txt&#39;)] . The images folder contains the images, and the corresponding segmentation masks of labels are in the labels folder. The codes file contains the corresponding &quot;integer to class&quot; mapping. . codes = np.loadtxt(path/&quot;codes.txt&quot;, dtype=str) codes . array([&#39;Animal&#39;, &#39;Archway&#39;, &#39;Bicyclist&#39;, &#39;Bridge&#39;, &#39;Building&#39;, &#39;Car&#39;, &#39;CartLuggagePram&#39;, &#39;Child&#39;, &#39;Column_Pole&#39;, &#39;Fence&#39;, &#39;LaneMkgsDriv&#39;, &#39;LaneMkgsNonDriv&#39;, &#39;Misc_Text&#39;, &#39;MotorcycleScooter&#39;, &#39;OtherMoving&#39;, &#39;ParkingBlock&#39;, &#39;Pedestrian&#39;, &#39;Road&#39;, &#39;RoadShoulder&#39;, &#39;Sidewalk&#39;, &#39;SignSymbol&#39;, &#39;Sky&#39;, &#39;SUVPickupTruck&#39;, &#39;TrafficCone&#39;, &#39;TrafficLight&#39;, &#39;Train&#39;, &#39;Tree&#39;, &#39;Truck_Bus&#39;, &#39;Tunnel&#39;, &#39;VegetationMisc&#39;, &#39;Void&#39;, &#39;Wall&#39;], dtype=&#39;&lt;U17&#39;) . We can use get_image_files to grab all the filenames. . fnames = get_image_files(path/&quot;images&quot;) fnames[0] . Path(&#39;/root/.fastai/data/camvid_tiny/images/Seq05VD_f00210.png&#39;) . Looking into the labels folder, we can see that the masks have the same base name as the images with a suffix _P . (path/&quot;labels&quot;).ls()[0] . Path(&#39;/root/.fastai/data/camvid_tiny/labels/0016E5_06300_P.png&#39;) . We can therefore easily define our label function. . def label_func(fn): return path/&quot;labels&quot;/f&quot;{fn.stem}_P{fn.suffix}&quot; . We can then create our dataloader using SegmentationDataLoaders. We are not passing any transforms as our images are all of the same size, being from the same source. . Using Factory Methods . dls = SegmentationDataLoaders.from_label_func( path, bs=8, fnames=fnames, label_func=label_func, codes=codes) . dls.show_batch(max_n=6) . A traditional CNN doesn&#39;t work for segmentation; we use a special kind of model called a UNet. So we use unet_learner to create our Learner. . learn = unet_learner(dls, resnet34) learn.fine_tune(6) . epoch train_loss valid_loss time . 0 | 2.905566 | 2.016695 | 00:04 | . epoch train_loss valid_loss time . 0 | 1.799236 | 1.573274 | 00:01 | . 1 | 1.541558 | 1.293306 | 00:01 | . 2 | 1.350324 | 1.001553 | 00:01 | . 3 | 1.186754 | 0.941967 | 00:01 | . 4 | 1.067865 | 0.868811 | 00:01 | . 5 | 0.972798 | 0.867281 | 00:01 | . learn.show_results(max_n=4) . interp = SegmentationInterpretation.from_learner(learn) interp.plot_top_losses(k=3) . Using the DataBlock API . The DataBlock object is itself empty and only contains the instructions on how to create your dataloader: . the data types, specified using blocks; here we pass ImageBlock and MaskBlock | how to retrieve the raw data; here we use get_image_files | how to retrieve the labels; here we use label_func | how to split the dataset; here we use RandomSplitter | how to transform the raw data; here we use only batch_tfms for image augmentation | . camvid = DataBlock(blocks = (ImageBlock, MaskBlock), get_items = get_image_files, get_y = label_func, splitter = RandomSplitter(), batch_tfms = aug_transforms(size=(120,160))) . dls = camvid.dataloaders(path/&quot;images&quot;, path=path, bs=8) . dls.show_batch(max_n=6) . Footnotes . 1. Placeholder↩ .",
            "url": "https://m1tr.github.io/helloworld/fastbook/jupyter/fastai/2021/08/30/ch-X-cv-segmentation.html",
            "relUrl": "/fastbook/jupyter/fastai/2021/08/30/ch-X-cv-segmentation.html",
            "date": " • Aug 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "My Fast.ai Journey - Classification",
            "content": "Welcome to the Classification notebook. . from fastai.vision.all import * . Single-label classification . Cats v Dogs . We will use the Oxford-IIIT Pets Dataset which contains images of cats and dogs of 37 different breeds. We first build a basic cats-vs-dogs classifier and later we will build a model that can distinguish different breeds. . path = untar_data(URLs.PETS) . path.ls() . (#3) [Path(&#39;/root/.fastai/data/oxford-iiit-pet/images&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/annotations&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/models&#39;)] . Ignoring the annotations folder for now, we use a helper function called get_image_files which allows us to grab all image files from a folder (recursively). get_image_files returns a list (actually it is an L object from the fastai foundations library) containing the paths to the individual images. . files = get_image_files(path/&quot;images&quot;) . files . (#7390) [Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/american_bulldog_56.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Siamese_61.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/english_cocker_spaniel_66.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/shiba_inu_55.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/scottish_terrier_68.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_153.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Ragdoll_57.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/pug_182.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Siamese_182.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/newfoundland_100.jpg&#39;)...] . The dataset follows a naming convention where the filenames for cat images begins with an uppercase letter whereas those for dog images are entirely in lowercase. We can write a quick function that performs this distinction automatically. . def label_func(f): return &quot;cat&quot; if f[0].isupper() else &quot;dog&quot; . Now we create a DataLoaders object. . Resizing is necessary at this stage since all images in a batch must be of same size to create our stack of tensors. . The from_name_func factory method expects a function to generate labels from filenames. . dls1 = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224)) . To verify if our dataloader has been created correctly, we use show_batch to return a batch of files and labels. . dls1.show_batch() . Now we create a Learner object which combines the data and the model for training. We use the cnn_learner factory method for convenience. . First we download a model called ResNet34, which has been pretrained on the ImageNet dataset and then fine-tune it on our dataset for one epoch. . learn1 = cnn_learner(dls1, resnet34, metrics=error_rate) . learn1.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.124347 | 0.026016 | 0.004736 | 00:24 | . epoch train_loss valid_loss error_rate time . 0 | 0.050945 | 0.019563 | 0.003383 | 00:30 | . We can now use the same Learner object for inference as well. . learn1.predict(files[0]), learn1.predict(files[7000]) . ((&#39;dog&#39;, tensor(1), tensor([2.1911e-05, 9.9998e-01])), (&#39;cat&#39;, tensor(0), tensor([1.0000e+00, 8.0579e-15]))) . predict returns the decoded prediction (dog or cat), the index of the predicted class (0 for cat, 1 for dog), and the probabilities of all the classes; here we can see that our model is quite confident of its predictions as it returns 1 for the predicted class. . Like with show_batch for the dataloader, we can use show_results on the learner to see a batch of images and the corresponding predictions. . learn1.show_results() . fastai also provides tools to interpret the model. See how we can find out those images for which the model made wrong predictions. . interp1 = Interpretation.from_learner(learn1) . interp1.plot_top_losses(12, figsize=(15,11)) . Classifying Animal Breeds . To generate labels with the name of the breed, we&#39;ll need to extract it from the filename. Regex will do the trick. . files[0].name . &#39;american_bulldog_56.jpg&#39; . pat = r&#39;^(.*)_ d+.jpg&#39; . Now we create the dataloader just like before, but we use the from_name_re factory method which expects a regex query to generate labels. . Also, we will use data augmentation to increase model performance without requiring additional data. Note that we use presizing to maintain image quality while augmenting. . dls2 = ImageDataLoaders.from_name_re(path, files, pat, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224)) . As earlier, we use show_batch to see if our labels match our data like we expect it to. . dls2.show_batch() . Now we create our learner as before and train our model. . learn2 = cnn_learner(dls2, resnet34, metrics=error_rate) . learn2.lr . 0.001 . We&#39;ve been using the default hyperparameters till now. Let&#39;s use the learning rate finder to find the most optimal value for the LR. The valley algorithm tends to, in general, provide good values. . lr = learn2.lr_find() . learn2.fine_tune(4, lr.valley) . epoch train_loss valid_loss error_rate time . 0 | 1.748438 | 0.365730 | 0.115020 | 00:28 | . epoch train_loss valid_loss error_rate time . 0 | 0.480563 | 0.298018 | 0.101489 | 00:34 | . 1 | 0.376828 | 0.272560 | 0.087280 | 00:34 | . 2 | 0.257416 | 0.226261 | 0.076455 | 00:34 | . 3 | 0.161659 | 0.213753 | 0.071719 | 00:34 | . learn2.show_results() . We can also use Interpretation to find the images for which the model made bad predictions. . interp2 = Interpretation.from_learner(learn2) . interp2.plot_top_losses(9, figsize=(15,10)) . Using the DataBlock API . If you need more flexibility than what the factory methods like ImageDataLoaders provide, you can create your dataloader using the DataBlock API directly. . The DataBlock object is itself empty and only contains the instructions on how to create your dataloader: . the data types, specified using blocks; here we pass ImageBlock and CategoryBlock | how to retrieve the raw data; here we use get_image_files | how to retrieve the labels; here we use RegexLabeller | how to split the dataset; here we use RandomSplitter | how to transform the raw data; here we use item_tfms and batch_tfms | . pets = DataBlock(blocks = (ImageBlock,CategoryBlock), get_items = get_image_files, splitter = RandomSplitter(), get_y = using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms = Resize(460), batch_tfms = aug_transforms(size=224)) . To actually generate a DataLoaders object, we need to call the dataloaders method on the DataBlock object. . dls3 = pets.dataloaders(untar_data(URLs.PETS)/&quot;images&quot;) . dls3.show_batch() . Multi-Label Classification . For this usecase, we will use the PASCAL dataset, originally designed for object detection. Let&#39;s begin by downloading the dataset and taking a look at this structure. . path = untar_data(URLs.PASCAL_2007) . . 100.00% [1637801984/1637796771 01:55&lt;00:00] path . Path(&#39;/root/.fastai/data/pascal_2007&#39;) . The label information is stored in train.csv. Let&#39;s take a look. . df = pd.read_csv(path/&quot;train.csv&quot;) df.head() . fname labels is_valid . 0 000005.jpg | chair | True | . 1 000007.jpg | car | True | . 2 000009.jpg | horse person | True | . 3 000012.jpg | car | False | . 4 000016.jpg | bicycle | True | . Using Factory Methods . Since our labels are stored in a Pandas DataFrame, we can use the from_df factory method from ImageDataLoaders. Few things to note here: . label_delim must be specified according to the source CSV file | folder specifies the additional folder between the filenames and the base path | valid_col for the validation set, if left empty a random split is created | fn_col for the filenames, defaults to the first column | label_col for the labels, defaults to the second column | . dls1 = ImageDataLoaders.from_df(df, path, folder=&#39;train&#39;, valid_col=&#39;is_valid&#39;, label_delim=&#39; &#39;, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224)) . dls1.show_batch() . We train our model as before, but for multi-label classification we need new metrics. Instead of error_rate, we use accuracy_multi and an additional thresh parameter. . learn1 = cnn_learner(dls1, resnet50, metrics=partial(accuracy_multi, thresh=0.5)) . Downloading: &#34;https://download.pytorch.org/models/resnet50-0676ba61.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth . . lr = learn1.lr_find() . learn1.fine_tune(4, lr.valley) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.908528 | 0.638845 | 0.672291 | 00:30 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.738822 | 0.568176 | 0.730956 | 00:37 | . 1 | 0.667090 | 0.463086 | 0.821096 | 00:36 | . 2 | 0.586458 | 0.395906 | 0.885538 | 00:36 | . 3 | 0.534789 | 0.388561 | 0.895179 | 00:36 | . learn1.show_results() . learn1.predict(path/&#39;train/000005.jpg&#39;) . ((#4) [&#39;chair&#39;,&#39;diningtable&#39;,&#39;sofa&#39;,&#39;tvmonitor&#39;], tensor([False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, True, False, True]), tensor([0.2113, 0.3219, 0.2255, 0.4994, 0.4806, 0.4571, 0.0190, 0.2152, 0.9999, 0.2971, 0.9497, 0.0827, 0.3972, 0.3270, 0.1272, 0.3067, 0.3307, 0.8221, 0.2460, 0.5876])) . Similar to single-label classification, we get the predicted labels and the probabilities for each class in the dataset. Note that we specified our threshold to be 0.5, so a probability higher than that will be marked as true. . We can also use Interpretation to find where our model performed worst. . interp1 = Interpretation.from_learner(learn1) interp1.plot_top_losses(9) . target predicted probabilities loss . 0 chair;person | boat;bus;chair;pottedplant;sofa;tvmonitor | tensor([0.2703, 0.2151, 0.2277, 0.5700, 0.2793, 0.6063, 0.1909, 0.1554, 0.9872, n 0.2907, 0.1827, 0.3841, 0.3053, 0.3447, 0.0107, 0.6876, 0.3070, 1.0000, n 0.4779, 0.6239]) | 1.1428658962249756 | . 1 motorbike | aeroplane;bicycle;motorbike | tensor([0.9990, 0.9996, 0.1331, 0.4047, 0.3638, 0.3510, 0.0131, 0.1392, 0.3583, n 0.3188, 0.3195, 0.0372, 0.1690, 0.9797, 0.1325, 0.1404, 0.3733, 0.3348, n 0.4743, 0.1922]) | 0.9979325532913208 | . 2 motorbike | car;cow;motorbike;person | tensor([0.0419, 0.1336, 0.1720, 0.4521, 0.1380, 0.4710, 1.0000, 0.1751, 0.1792, n 0.5761, 0.4756, 0.2181, 0.1482, 0.9999, 0.8565, 0.2730, 0.3051, 0.4091, n 0.1249, 0.1561]) | 0.9831724166870117 | . 3 dog | boat;cat;chair;diningtable;dog;pottedplant;sofa;tvmonitor | tensor([0.2514, 0.3774, 0.1430, 0.5729, 0.4942, 0.2068, 0.1746, 0.9382, 0.9751, n 0.2904, 0.6993, 0.9917, 0.4175, 0.1450, 0.4664, 0.7856, 0.1786, 0.9766, n 0.0455, 0.7090]) | 0.9501069188117981 | . 4 dog | cat;chair;diningtable;horse;sofa | tensor([0.1726, 0.1877, 0.1816, 0.1666, 0.3339, 0.3447, 0.4429, 1.0000, 0.6089, n 0.3783, 0.5191, 0.4010, 0.5178, 0.4054, 0.0165, 0.1276, 0.2541, 0.7488, n 0.0880, 0.1823]) | 0.936911404132843 | . 5 diningtable;pottedplant;sofa | bottle;chair;diningtable;person;pottedplant;sofa;tvmonitor | tensor([0.1000, 0.2872, 0.0966, 0.3727, 0.9444, 0.3072, 0.0968, 0.1238, 0.9998, n 0.4412, 0.9998, 0.0996, 0.3315, 0.2606, 0.6076, 0.6626, 0.2542, 0.8497, n 0.2235, 0.5521]) | 0.8698863983154297 | . 6 car | car;motorbike;train | tensor([0.2359, 0.1015, 0.1074, 0.3456, 0.4648, 0.2353, 0.7897, 0.1124, 0.1502, n 0.3035, 0.1825, 0.2050, 0.2844, 1.0000, 0.3567, 0.3161, 0.1771, 0.1877, n 0.6062, 0.3358]) | 0.8565415740013123 | . 7 bus;person | bus;car;motorbike | tensor([0.2596, 0.4596, 0.1680, 0.1980, 0.2626, 0.9487, 0.9999, 0.2214, 0.3765, n 0.2269, 0.3139, 0.2327, 0.3281, 0.5234, 0.1653, 0.2959, 0.3655, 0.3631, n 0.2379, 0.2834]) | 0.8542322516441345 | . 8 dog | diningtable;dog;horse;person;sofa;tvmonitor | tensor([0.0473, 0.2148, 0.0983, 0.3255, 0.4347, 0.1502, 0.0514, 0.0930, 0.3215, n 0.3373, 0.7067, 0.9632, 0.9996, 0.1712, 0.8572, 0.2182, 0.4595, 0.5212, n 0.4161, 0.6747]) | 0.8538174629211426 | . Using the DataBlock API . Like before, we can also use the mid-level API directly. To create our DataBlock object, we specify: . the data types, specified using blocks; here we pass ImageBlock and MultiCategoryBlock | how to retrieve the input items from our dataframe; here we read the column fname and prepend the correct dirnames | how to retrieve the labels; here we read the column labels and use whitespace as a delimiter | how to split the dataset; here we use the column is_valid | how to transform the raw data; here we use item_tfms and batch_tfms | . pascal = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=ColSplitter(&#39;is_valid&#39;), get_x=ColReader(&#39;fname&#39;, pref=str(path/&#39;train&#39;) + os.path.sep), get_y=ColReader(&#39;labels&#39;, label_delim=&#39; &#39;), item_tfms = Resize(460), batch_tfms=aug_transforms(size=224)) . As before, to actually create our DataLoaders object, we need to call the dataloaders method on the DataBlock object. . dls2 = pascal.dataloaders(df) . dls2.show_batch(max_n=9) . Segmentation . Segmentation is similar to classification, except that we classify each individual pixel in the image. We&#39;ll use the CAMVID dataset which contains images from a car dashcam. Let&#39;s extract the dataset and take a look. . path = untar_data(URLs.CAMVID_TINY) path.ls() . (#3) [Path(&#39;/root/.fastai/data/camvid_tiny/labels&#39;),Path(&#39;/root/.fastai/data/camvid_tiny/images&#39;),Path(&#39;/root/.fastai/data/camvid_tiny/codes.txt&#39;)] . The images folder contains the images, and the corresponding segmentation masks of labels are in the labels folder. The codes file contains the corresponding &quot;integer to class&quot; mapping. . codes = np.loadtxt(path/&quot;codes.txt&quot;, dtype=str) codes . array([&#39;Animal&#39;, &#39;Archway&#39;, &#39;Bicyclist&#39;, &#39;Bridge&#39;, &#39;Building&#39;, &#39;Car&#39;, &#39;CartLuggagePram&#39;, &#39;Child&#39;, &#39;Column_Pole&#39;, &#39;Fence&#39;, &#39;LaneMkgsDriv&#39;, &#39;LaneMkgsNonDriv&#39;, &#39;Misc_Text&#39;, &#39;MotorcycleScooter&#39;, &#39;OtherMoving&#39;, &#39;ParkingBlock&#39;, &#39;Pedestrian&#39;, &#39;Road&#39;, &#39;RoadShoulder&#39;, &#39;Sidewalk&#39;, &#39;SignSymbol&#39;, &#39;Sky&#39;, &#39;SUVPickupTruck&#39;, &#39;TrafficCone&#39;, &#39;TrafficLight&#39;, &#39;Train&#39;, &#39;Tree&#39;, &#39;Truck_Bus&#39;, &#39;Tunnel&#39;, &#39;VegetationMisc&#39;, &#39;Void&#39;, &#39;Wall&#39;], dtype=&#39;&lt;U17&#39;) . We can use get_image_files to grab all the filenames. . fnames = get_image_files(path/&quot;images&quot;) fnames[0] . Path(&#39;/root/.fastai/data/camvid_tiny/images/Seq05VD_f00210.png&#39;) . Looking into the labels folder, we can see that the masks have the same base name as the images with a suffix _P . (path/&quot;labels&quot;).ls()[0] . Path(&#39;/root/.fastai/data/camvid_tiny/labels/0016E5_06300_P.png&#39;) . We can therefore easily define our label function. . def label_func(fn): return path/&quot;labels&quot;/f&quot;{fn.stem}_P{fn.suffix}&quot; . We can then create our dataloader using SegmentationDataLoaders. We are not passing any transforms as our images are all of the same size, being from the same source. . dls = SegmentationDataLoaders.from_label_func( path, bs=8, fnames=fnames, label_func=label_func, codes=codes) . dls.show_batch(max_n=6) . A traditional CNN doesn&#39;t work for segmentation; we use a special kind of model called a UNet. So we use unet_learner to create our Learner. . learn = unet_learner(dls, resnet34) learn.fine_tune(6) . epoch train_loss valid_loss time . 0 | 2.827345 | 2.343988 | 00:01 | . epoch train_loss valid_loss time . 0 | 1.605639 | 1.447259 | 00:02 | . 1 | 1.454939 | 1.168440 | 00:02 | . 2 | 1.310351 | 0.962879 | 00:01 | . 3 | 1.165811 | 0.897058 | 00:01 | . 4 | 1.051536 | 0.796033 | 00:02 | . 5 | 0.964535 | 0.783767 | 00:02 | . learn.show_results(max_n=4) . interp = SegmentationInterpretation.from_learner(learn) interp.plot_top_losses(k=3) . Footnotes . 1. Placeholder↩ .",
            "url": "https://m1tr.github.io/helloworld/fastbook/jupyter/fastai/2021/08/24/ch-X-cv-classification.html",
            "relUrl": "/fastbook/jupyter/fastai/2021/08/24/ch-X-cv-classification.html",
            "date": " • Aug 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "My Fast.ai Journey - Learning the Basics",
            "content": "In this post, we&#39;ll look at the basic theory of machine learning and define a glossary of machine learning jargon. . Machine Learning . Let&#39;s start at the beginning, with the definition of machine learning, coined by an IBM researcher named Arthur Samuel. In his classic 1962 essay Artificial Intelligence: A Frontier of Automation1, he described the basic idea that instead of programming a computer with the exact steps to complete a certain task, showing the computer examples of completed tasks will allow it to figure out how to get there on its own! In essence, the computer itself defines the steps it needs to take to transform the inputs into desired outputs; it writes its own program. . Samuels specified the basic idea of a training loop where a system is configured with a set of parameters and we have some way of testing the effectiveness of this configuration by observing the output of the system. We also need a way of automatically tweaking or tuning these parameters so that the output of the system keep improving. This, essentially, is a method using which a system can learn how to perform a task without any explicit instructions or programming. The system starts out with a random transformation and incrementally improves it to be able to transform the inputs into correct outputs. . Today, the following process forms the heart of any machine learning algorithm: . &lt; insert image A &gt; . The training loop The functional form or structure of our transformation is called the architecture, it&#39;s a program, a mathematical function that converts our inputs into some output. | The architecture has tunable knobs called parameters that change the way it processes inputs. Weights are a kind of parameter. | The architecture and parameters together comprise the model. | The output of the model are its predictions, which are calculated from the independent variables, which is the data without the labels. | The measure of a model&#39;s performance is called the loss. The lower the loss, the better. | The loss depends on both the predictions and the labels (also known as targets or dependent variables). | The process of updating the parameters based on the loss is called training. | . When we are happy with our model&#39;s predictions or the loss value is sufficiently low, our training is complete. We then stop updating the parameters and calculating the loss and instead use the predictions directly; this is called inference and is what happens in production. . Neural Networks . Now that we have an idea about how to design a system which can &quot;learn&quot;, or improve its performance, on its own, it&#39;s time to see how exactly the system transforms inputs into outputs; essentially, we want to learn more about the architecture block. We want a mathematical function that is so flexible that it can, in theory, learn to transform any artitrary input into any arbitrary output. As it turns out, such a function does exist; it&#39;s called a neural network. . In the influential two volume Parallel Distributed Processing: Explorations in the Microstructure of Cognition by David Rumelhart, James McClellan, and the PDP Research Group, published in 1986, a &quot;computational framework for modelling cognitive processes&quot; was introduced, inspired by the way neurons in organic brains operate. The idea here is that since traditional computer programs do not operate the way brains do, they also fail at tasks which are easy for the latter, such as recognising objects in pictures. PDP did not invent neural networks, but their approach laid the foundations that led to modern neural networks. . 2According to PDP, to achieve &quot;parallel distributed processing&quot;, you need: . A set of processing units which accept inputs, or neurons | A state of activation for each unit | An output function for each unit | A pattern of connectivity among units, i.e. a network between the units | A propagation rule for propagating patterns of activities throughout the network | An activation rule for combining the inputs to a unit with the current state of that unit to produce an output | A learning rule whereby patterns of connectivity can be modified over time | An environment within which the system operates | &lt; insert image of neurons, neural networks &gt; . And so a neural network is defined: a network of neurons that can take inputs and produce outputs. The behaviour of the network can be modified or tuned by modifying its parameters. Most modern neural networks are multi-layered, meaning that the outputs of a set of neurons are used as inputs for another set of neurons. According to a mathematical proof called the Universal Approximation Theorem, a neural network is flexible enough to model any arbitrary mathematical function. . Let&#39;s refer back to our training loop: . &lt; insert image A &gt; . Now that we have our &quot;architecture&quot; in place, we need a general way to update the parameters of the network, i.e. optimise it. Fortunately, we have a process for this as well, the most fundamental of which is called stochastic gradient descent or SGD. We use SGD to optimise our network based on the loss of the network. The loss, computed by a loss function is a measure of how far off the output of the network was from the correct response; the lower the loss, the better our network performs. . And that&#39;s all there is to it: a neural network, an optimiser, and a loss function. Given enough data, this combination can learn to solve almost any task you provide. . Datasets . garbage-in-garbage-out, Structuring data, train vs validation, creating good val sets, feedback loops . Memorising vs Learning . A delicate balance. . Categories of usecases . regression, classification, segmentation, obj det, nlp . Transfer Learning . deep learning and transfer learning . Types of networks . overview of the major types of neural network architectures and their uses . Statistcal Methods vs Deep Learning . what to use and when . Miscellaneous Terminology . bias vs variance features . Footnotes . 1. Placeholder↩ . 2. Fastbook Chapter 1↩ .",
            "url": "https://m1tr.github.io/helloworld/fastbook/jupyter/fastai/2021/08/16/ch-1-the-basics.html",
            "relUrl": "/fastbook/jupyter/fastai/2021/08/16/ch-1-the-basics.html",
            "date": " • Aug 16, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "My Fast.ai Journey - Chapter 0",
            "content": "I&#39;ve done it! I&#39;ve finally completed Part 1 of the Fast.ai course. . It took me some time, what with working full time and the numerous psychological setbacks that come with being confined to one&#39;s house as less fortunate people bear the brunt of a global pandemic, but I persevered. Another major goal I had was to document my learning journey and start a blog. I haven&#39;t really spared much thought towards the intent of the blog or its audience but for now, I&#39;m going to simply prepare a basic (read: beginner-level) reference guide to the Fast.ai library and how to perform some common machine learning usecases using it. . I&#39;ll write additional articles as I dive deeper into Fast.ai and machine learning in general but if you are an advanced practitioner, expect this series to be pretty boring! However, if you have constructive criticism, I&#39;m always happy to hear it. . Once I complete this series of articles, my plan is to start working on projects. I am not planning on deriving state-of-the-art results or tackling global warming, but build something fun and interesting which will give me an excuse to write code everyday and sharpen my Google-fu skills! . Right now, I have two projects in mind: . Battery Health Prediction using NASA PCoE Battery Data | Novel Bengali Poetry (i.e. pick-up lines) Generator using Free Bengali Poetry dataset by Ritobrata Ghosh | . When (and if) inspiration strikes, I&#39;ll add other potential project ideas here. I might decide to look into some medical usecases like radiography or blood film analysis later on. . Here&#39;s the ToC for the series. Hope to see you around! . Placeholder | Placeholder | Placeholder | . There&#39;s no need to sharpen my pencils anymore, my pencils are sharp enough; even the dull ones will make a mark. . Warts and all. . Let&#39;s start this shit up!1 . . Footnotes . 1. Ze Frank, An Invocation for Beginnings↩ .",
            "url": "https://m1tr.github.io/helloworld/fastbook/jupyter/fastai/2021/08/16/ch-0-intro.html",
            "relUrl": "/fastbook/jupyter/fastai/2021/08/16/ch-0-intro.html",
            "date": " • Aug 16, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://m1tr.github.io/helloworld/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://m1tr.github.io/helloworld/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "My Fast.ai Journey - Post Title",
            "content": "Hello there1 Remove hide: true to make post visible . Footnotes . 1. Placeholder↩ .",
            "url": "https://m1tr.github.io/helloworld/fastbook/jupyter/fast.ai/2020/01/01/post-template.html",
            "relUrl": "/fastbook/jupyter/fast.ai/2020/01/01/post-template.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m interested in becoming a Machine Learning Engineer. Please check out my resume here. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://m1tr.github.io/helloworld/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://m1tr.github.io/helloworld/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}