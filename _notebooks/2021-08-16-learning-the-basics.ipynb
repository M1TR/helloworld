{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Fast.ai Journey - Learning the Basics\n",
    "> An brief overview of the fundamentals of machine learning\n",
    "\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastbook, jupyter, fast.ai]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we'll look at the basic theory of machine learning and define a glossary of machine learning jargon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "Let's start at the beginning, with the definition of *machine learning*, coined by an IBM researcher named Arthur Samuel. In his classic 1962 essay *Artificial Intelligence: A Frontier of Automation{% fn 1 %}*, he described the basic idea that instead of programming a computer with the exact steps to complete a certain task, *showing* the computer examples of completed tasks will allow it to figure out how to get there on its own! In essence, the computer itself defines the steps it needs to take to transform the inputs into desired outputs; it writes its own program.\n",
    "\n",
    "Samuels specified the basic idea of a **training loop** where a system is configured with a **set of parameters** and we have some way of *testing the effectiveness* of this configuration by observing the output of the system. We also need a way of automatically tweaking or **tuning** these parameters so that the output of the system keep improving. This, essentially, is a method using which a system can **learn** how to perform a task without any explicit instructions or programming. The system starts out with a random transformation and incrementally improves it to be able to transform the inputs into correct outputs.\n",
    "\n",
    "Today, the following process forms the heart of any machine learning algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< insert image A >\n",
    "\n",
    "<center><font size='2'><i>The training loop</i></font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The functional form or *structure* of our transformation is called the **architecture**, it's a program, a mathematical function that converts our inputs into some output. \n",
    "- The architecture has tunable knobs called **parameters** that change the way it processes inputs. **Weights** are a kind of parameter.\n",
    "- The architecture and parameters together comprise the **model**.\n",
    "- The output of the model are its **predictions**, which are calculated from the **independent variables**, which is the data *without the labels*.\n",
    "- The measure of a model's performance is called the **loss**. The lower the loss, the better.\n",
    "- The loss depends on both the predictions and the labels (also known as **targets** or **dependent variables**).\n",
    "- The process of updating the parameters based on the loss is called **training**.\n",
    "\n",
    "When we are happy with our model's predictions *or* the loss value is sufficiently low, our training is complete. We then stop updating the parameters and calculating the loss and instead use the predictions directly; this is called **inference** and is what happens in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "Now that we have an idea about how to design a system which can \"learn\", or improve its performance, on its own, it's time to see how exactly the system transforms inputs into outputs; essentially, we want to learn more about the *architecture* block. We want a mathematical function that is so flexible that it can, in theory, learn to transform any artitrary input into any arbitrary output. As it turns out, such a function does exist; it's called a neural network.\n",
    "\n",
    "In the influential two volume *Parallel Distributed Processing: Explorations in the Microstructure of Cognition* by David Rumelhart, James McClellan, and the PDP Research Group, published in 1986, a \"computational framework for modelling cognitive processes\" was introduced, inspired by the way neurons in organic brains operate. The idea here is that since traditional computer programs do not operate the way brains do, they also fail at tasks which are easy for the latter, such as recognising objects in pictures. PDP did not invent neural networks, but their approach laid the foundations that led to modern neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{% fn 2 %}According to PDP, to achieve \"parallel distributed processing\", you need:\n",
    "1. A set of *processing units* which accept inputs, or *neurons*\n",
    "2. A *state of activation* for each unit\n",
    "3. An *output function* for each unit\n",
    "4. A *pattern of connectivity* among units, i.e. a network between the units\n",
    "5. A *propagation rule* for propagating patterns of activities throughout the network\n",
    "6. An *activation rule* for combining the inputs to a unit with the current *state* of that unit to produce an output\n",
    "7. A *learning rule* whereby patterns of connectivity can be modified over time\n",
    "8. An *environment* within which the system operates\n",
    "\n",
    "< insert image of neurons, neural networks > \n",
    "\n",
    "And so a neural network is defined: a *network* of *neurons* that can take inputs and produce outputs. The behaviour of the network can be modified or tuned by modifying its *parameters*. Most modern neural networks are multi-layered, meaning that the outputs of a set of neurons are used as inputs for another set of neurons. According to a mathematical proof called the *Universal Approximation Theorem*, a neural network is flexible enough to model any arbitrary mathematical function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refer back to our training loop: \n",
    "\n",
    "< insert image A >\n",
    "\n",
    "Now that we have our \"architecture\" in place, we need a general way to update the parameters of the network, i.e. *optimise* it. Fortunately, we have a process for this as well, the most fundamental of which is called *stochastic gradient descent* or *SGD*. We use SGD to optimise our network based on the *loss* of the network. The loss, computed by a *loss function* is a measure of how far off the output of the network was from the correct response; the lower the loss, the better our network performs.\n",
    "\n",
    "And that's all there is to it: a neural network, an optimiser, and a loss function. Given enough data, this combination can learn to solve almost any task you provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memorising vs Learning\n",
    "\n",
    "A delicate balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "deep learning and transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "garbage-in-garbage-out, Structuring data, train vs validation, creating good val sets, feedback loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories of usecases\n",
    "\n",
    "regression, classification, segmentation, obj det, nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of networks\n",
    "\n",
    "overview of the major types of neural network architectures and their uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistcal Methods vs Deep Learning\n",
    "\n",
    "what to use and when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "{{ '[Placeholder](placeholder)' | fndetail: 1 }}\n",
    "\n",
    "{{ '[Fastbook Chapter 1](https://github.com/fastai/fastbook/blob/master/01_intro.ipynb)' | fndetail: 2 }}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
